{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS345 Fall 2024 Assignment 2\n",
    "\n",
    "Updated 9/13/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "We'll start with a review of the notation used to represent a labeled dataset. In supervised learning we work with a dataset of $N$ labeled examples, each of which is a pair $(\\mathbf{x}_i, y_i)$, where $\\mathbf{x}_i$ is a $d$-dimensional vector (we always use boldface to denote vectors), and $y_i$ is the label associated with $\\mathbf{x}_i$.  Keep in mind that the formulation of the perceptron algorithm that we used in class relies on the labels being $\\pm 1$, so make sure that is the case for the data you use!\n",
    "\n",
    "In this assignment we will use the following datasets:\n",
    "\n",
    "\n",
    "* The [QSAR](https://archive.ics.uci.edu/dataset/254/qsar+biodegradation) data for predicting the biochemical activity of a molecule.\n",
    "* The [Wisconsin breast cancer dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer). \n",
    "* The [Gisette](https://archive.ics.uci.edu/dataset/170/gisette) handwritten digit recognition dataset. For this dataset you are provided separate training/validation/test sets.  Since the test set doesn't come with labels, use the validation set for testing the classifier.\n",
    "* The [heart disease diagnosis](https://archive.ics.uci.edu/dataset/45/heart+disease) dataset.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0:  Data Loaders\n",
    "\n",
    "Write data loaders for each of the four datasets listed above.  Use the same API as in assignment 1 for creating the penguins dataset, which mimics the data loaders provided by scikit-learn.\n",
    "\n",
    "For example, a function ```load_qsar``` should return a feature matrix X and labels vector y for this dataset.  Similarly for the other datasets.  For the ```load_breast_cancer``` function, you may use the scikit-learn function that creates the dataset.  Since the gisette dataset has separate training and validation sets (that you will use as training / test sets), you will need to write two data loaders - one for the training set, and one for the test set.  Note that you will need to convert the labels from the values 0,1 to $\\pm 1$, since that is what our perceptron learning algorithm expects as label values.\n",
    "\n",
    "#### Missing values\n",
    "Whenever a dataset has missing values, any training example that has missing features should be removed.  (An alternative to removing training examples is to *impute* missing features, e.g. by replacing missing values by the average of that feature.)\n",
    "\n",
    "#### File structure\n",
    "\n",
    "Store all the files you download in a sub-directory called `data` relative to the location of your notebook.  \n",
    "Make sure to use the filenames specified in each data loader.\n",
    "This will ensure that your code will run properly when we execute your notebook\n",
    "\n",
    "#### A note on the heart disease diagnosis dataset\n",
    "The heart disease diagnosis dataset has several data files associated with it.  Use [this file](http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data), where categorical variables have been replaced with numerical vaues.  The last column in the file contains the label associated with each example.  In the processed file, a label `0` corresponds to a healthy individual; other values correspond to varying levels of heart disease.  **In your experiments focus on the binary classification problem of trying to distinguish between healthy and non-healthy individuals.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the following data loaders.  \n",
    "# All the data files should be in a sub-folder called \"data\" relative to the\n",
    "# location of your notebook.\n",
    "# In each dataloader we specify the expected filenames you should use\n",
    "# to ensure your notebook runs correctly\n",
    "\n",
    "def load_qsar():\n",
    "    filename='data/biodeg.csv'\n",
    "    return np.array([]),np.array([])\n",
    "\n",
    "def load_breast_cancer():\n",
    "    # use the scikit-learn data loader\n",
    "    return np.array([]),np.array([])\n",
    "\n",
    "def load_gisette_train():\n",
    "    features_filename='data/gisette_train.data'\n",
    "    labels_filename='data/gisette_train.labels'\n",
    "    return np.array([]),np.array([])\n",
    "\n",
    "def load_gisette_test():\n",
    "    features_filename='data/gisette_valid.data'\n",
    "    labels_filename='data/gisette_valid.labels'    \n",
    "    return np.array([]),np.array([])\n",
    "\n",
    "def load_heart():\n",
    "    heart_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "    heart_data = pd.read_csv(heart_url, header=None, na_values='?', dtype=np.float64).values    \n",
    "    return np.array([]),np.array([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following function to check that your function returns arrays of the appropriate shapes (you will need to determine how many features / examples each dataset contains).  The only case where this is somewhat of a challenge is the heart dataset, which contains some missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validity for heart dataset:  False\n"
     ]
    }
   ],
   "source": [
    "def data_is_valid(X, y, examples=0, features=0):\n",
    "    return (\n",
    "        X.shape == (examples, features)\n",
    "        and y.shape == (examples,)\n",
    "        and not np.any(np.isnan(X))\n",
    "        and np.all((y==1) | (y==-1))\n",
    "    )\n",
    "\n",
    "# for example:\n",
    "heart_X, heart_y = load_heart()\n",
    "print(\"validity for heart dataset: \", data_is_valid(heart_X, heart_y, examples=297, features=13))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Evaluating the Perceptron on Real World Datasets\n",
    "\n",
    "In this part of the assignment you will work with the perceptron algorithm and run it on two real-world datasets.  For comparison, you will also evaluate an [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) classifier on the same datasets.  We will cover SVMs in detail later in the course, and here you will simply use it with its default settings.\n",
    "\n",
    "- Compare the performance of the  perceptron using the implementation we used in class with the SVM classifier on the QSAR and breast cancer diagnosis datasets. Do so by estimating the accuracy on a sample of the data that you reserve for testing (the test set).  In each case reserve  70% of the data for training, and 30% for testing.  To gain more confidence in your error estimates, repeat this experiment using 10 random splits of the data into training/test sets for each algorithm.  Use the same train-test splits for each algorithm.  Report the average accuracy and its standard deviation in a nicely formatted table.  Is there a classifier among the two that appears to perform better?  In answering this, consider the differences in performance you observe in comparison to the standard deviation.  Make sure to let the perceptron algorithm run for a sufficient number of epochs.  In implementing this task, you may use a for loop to iterate over the 10 random splits.\n",
    "\n",
    "A note about the classifier API:  in this course we follow the scikit-learn classifier API, which requires that a classifier have the following methods (in addition to a constructor):\n",
    "\n",
    "* `fit(X, y)`:  trains a classifier using a feature matrix `X` and a labels vector `y`.\n",
    "* `predict(X)`:  given a feature matrix `X`, return a vector of labels for each feature vector represented by `X`.\n",
    "\n",
    "For those interested in more information about the scikit-learn API, here's a [link](https://scikit-learn.org/stable/developers/develop.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about displaying your results\n",
    "\n",
    "We recommend displaying the results of your experiments in the form of an automatically-generated table.  pandas DataFrame objects render nicely in Jupyter notebooks, and are an easy way to achieve this with minimal work.  Here's an example that you can use as a template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Mean</th>\n",
       "      <th>StdDev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classifier  Mean  StdDev\n",
       "0  Perceptron     0       0\n",
       "1         SVM     0       0\n",
       "2  Perceptron     0       0\n",
       "3         SVM     0       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = [\n",
    "    ['Perceptron (cancer)', 0, 0],\n",
    "    ['SVM (cancer)', 0, 0],\n",
    "    ['Perceptron (biodeg)', 0, 0],\n",
    "    ['SVM (biodeg)', 0, 0],\n",
    "]\n",
    "pd.DataFrame(data, columns = ['Classifier', 'Mean', 'StdDev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# use an SVM classifier with default settings to compare to the\n",
    "# perceptron:\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*discussion of your results*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Learning Curves \n",
    "\n",
    "Whenever we train a classifier it is useful to know if we have collected a sufficient amount of data for accurate classification.  A good way of determining that is to construct a **learning curve**, which is a plot of classifier performance as a function of the number of training examples.  Plot a learning curve for the perceptron algorithm using the [Gisette](http://archive.ics.uci.edu/dataset/170/gisette) handwritten digit recognition dataset. For this dataset use the separately provided validation set for testing your classifiers.  A test set is provided without its labels, so is not usable for us.\n",
    "The x-axis for the plot (number of training examples) should be on a logarithmic scale - something like 10,20,40,80,200,400,800 etc.  In your submission use numbers that are appropriate for the dataset at hand.  Since the x-axis is on a logarithmic scale, plot the learning curve using [plt.semilogx](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.semilogx.html).\n",
    "\n",
    "What can you conclude from the learning curve you have constructed for this particular dataset?\n",
    "\n",
    "In answering this question, you can use the following [wikipedia article](https://en.wikipedia.org/wiki/Learning_curve#In_machine_learning).\n",
    "Make sure that you use a fixed test set to evaluate performance while varying the size of the training set.  Use the Gisette validation set for that purpose.\n",
    "Also, do not use the scikit-learn function for computing the validation curve, or any other scikit-learn functions for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*discussion of your results*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Data standardization \n",
    "\n",
    "In this section we will explore the effect of normalizing the data, focusing on normalization of each feature individually.  In class we saw how to convert each column (i.e. feature) of a data matrix so that it fall in the range $[-1,1]$.  In this assignment we will explore a different approach callled **standardization**.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "* Write a method to standardize a data matrix, so that each column has zero mean and standard deviation equal to 1.  This is done by subtracting the mean of each column, and dividing by its standard deviation.  See details [here](https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)).  Scikit-learn has a method called [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) which does this.  Do not use it!  To demonstrate that your method works correctly, show that after standardization, your feature matrix has a zero mean and standard deviation equal to 1 for each column.  Make sure not to use for loops!\n",
    "\n",
    "* Compare the accuracy of the standard perceptron on the heart dataset  with standardization and without it (make sure to evaluate the accuracy on a held out test set).  Like we did earlier, report the accuracy as the average over ten train-test splits.  Which leads to better performance?  Can you explain why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*discussion and explanation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4:  Use of AI and other web resources\n",
    "\n",
    "In the cell below indicate in detail how you used AI and other web resources for this assignment.  If you used AI tools, indicate how useful they were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Report\n",
    "\n",
    "Answer the questions in the cells reserved for that purpose.\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "Submit your report as a Jupyter notebook via Canvas.  Running the notebook should generate all the results and plots in your notebook.\n",
    "\n",
    "### Grading \n",
    "\n",
    "\n",
    "```\n",
    "Grading sheet for assignment 2\n",
    "Part 0:  20 points\n",
    "Part 1:  40 points\n",
    "Part 2:  20 points\n",
    "Part 3:  20 points\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
